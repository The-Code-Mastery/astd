```{r, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

## Areal / lattice data.

For typical lattice data, observed values are associated with
areas, and the union of the areas observed cover the region of
interest. The boundaries of the areas do not follow from the observed
phenomen's properties, but typically from external constraints to
the observation process.

Examples include:

* socio-economic data, related to administrative regions (census, administration)
* health data, related to hospital wards, or aggregated to adm. regions
* satellite imagery (aggregates, related to pixels)
* climate model predictions, as aggregates over regions

Problems include:

* estimate proportions or risks from count data: 
    * assess whether there are (contiguous) _areas_ with elevated risk
    * if this is the case, find correlations with other variables
* areal interpolation: interpolate (downscale/upscale) the observed values to some other units, that are hierchically related to the current areal units (e.g. landsat/modis, change of administrative regions, integration of health and census data)
* evaluating models (testing hypothesis) expressed as $y = X\beta + e$, where either $y$ and/or $e$ exhibit autocorrelation

Typical analysis (see R package `spdep`) follows a path:

* build, and evaluate a spatial weight matrix (based on neighbours, or distance, or both)
* test for spatial correlation using Moran's I
* give significant spatial correlation, build models that account for that


The following is heavily inspired by Ch 17 and 18 of https://r-spatial.org/book .

```{r fig.width=12,fig.height=12}
library(sf)
data(pol_pres15, package="spDataLarge")
head(pol_pres15[, c(1, 4, 6)])
library(spdep)
p = poly2nb(pol_pres15)
plot(st_geometry(pol_pres15), border = 'grey')
plot(p, st_centroid(st_geometry(pol_pres15)), pch = 3, cex = .2, add = TRUE)
```

Finding queen neighbours using sf:
```{r}
st_queen <- function(a, b = a) st_relate(a, b, pattern = "F***T****")
as.nb.sgbp <- function(x, ...) {
  attrs <- attributes(x)
  x <- lapply(x, function(i) { if(length(i) == 0L) 0L else i } )
  attributes(x) <- attrs
  class(x) <- "nb"
  x
}
nb_sf_q <- as.nb.sgbp(st_queen(pol_pres15))
nb_q <- poly2nb(pol_pres15, queen=TRUE)
all.equal(nb_q, nb_q, check.attributes=FALSE)
```

Moran's I:
$$
I = \frac{n \sum_{(2)} w_{ij} z_i z_j}{S_0 \sum_{i=1}^{n} z_i^2}
$$
where $x_i, i=1, \ldots, n$ are $n$ observations on the numeric variable of interest, $z_i = x_i - \bar{x}$, $\bar{x} = \sum_{i=1}^{n} x_i / n$, $\sum_{(2)} = \stackrel{\sum_{i=1}^{n} \sum_{j=1}^{n}}{i \neq j}$, $w_{ij}$ are the spatial weights, and $S_0 = \sum_{(2)} w_{ij}$. 

First we test a random variable using the Moran test, here under the normality assumption (argument `randomisation=FALSE`, default `TRUE`):

```{r}
x <- rnorm(nrow(pol_pres15))
(mt <- moran.test(x, nb2listw(nb_q), randomisation=FALSE))
```
The test variable is $Z(I)=(I-E(I))/\sqrt{(Var(I))}$, which is standard normally distributed under the $H_0$ of spatial independence.

Now try a real variable
```{r}
plot(pol_pres15["I_turnout"])
moran.test(pol_pres15$I_turnout, nb2listw(nb_q), randomisation=FALSE)
```
Alternatively, we could use a randomisation to compute $Var(I)$:
```{r}
moran.test(pol_pres15$I_turnout, nb2listw(nb_q), randomisation=TRUE)
```

or use a permutation test:
```{r}
moran.mc(pol_pres15$I_turnout, nb2listw(nb_q), nsim = 999)
```


In case we are curious whether this could be a trend in northing and easting, we could try
```{r}
cc = st_coordinates(st_centroid(pol_pres15))
pol_pres15$x = cc[,1]
pol_pres15$y = cc[,2]
lm.morantest(lm(I_turnout~x+y, pol_pres15), nb2listw(nb_q))
```

```{r}
library(spatialreg)
(e1 = errorsarlm(I_turnout~x+y, pol_pres15, nb2listw(nb_q)))
summary(e1)
(e2 = errorsarlm(I_turnout~types, pol_pres15, nb2listw(nb_q)))
summary(e2)
```

Models used:

* simultaneous autoregressive models (asdar 2nd ed, 9.4.1.1; spatial data science Ch 18)
* conditional autoregressive models (asdar 2nd ed, 9.4.1.2; spatial data science Ch 18)

Useful spdep vignettes:

* [creating neighbours](https://cran.r-project.org/web/packages/spdep/vignettes/nb.pdf)
* [Introduction to the North Carolina SIDS dataset](https://cran.r-project.org/web/packages/spdep/vignettes/sids.pdf)
* [Spatial weights objects as sparse matrices and graphs](https://cran.r-project.org/web/packages/spdep/vignettes/nb_igraph.html)
* ["The Problem of Spatial Autocorrelation:" forty years on](https://cran.r-project.org/web/packages/spdep/vignettes/CO69.pdf)

More modern approaches, using R-INLA:

* [Spatial Data Analysis with R-INLA with Some Extensions](https://www.jstatsoft.org/article/view/v063i20)

# Exercises

## Produc data

Load the produc dataset by
```{r, eval = FALSE}
# get US state delineations:
library(maps) 
states.m = map('state', plot=FALSE, fill=TRUE)
IDs <- sapply(strsplit(states.m$names, ":"), function(x) x[1])
library(maptools)
states = map2SpatialPolygons(states.m, IDs=IDs)
# get the Produc dataset from plm:
library(plm)
data(Produc)
# convert into sf object:
library(sf)
Produc86 = subset(Produc, year == 1986)
st_geometry(Produc86) = st_as_sfc(states[-8])
plot(Produc86)
```

* What does variable `gsp` mean in this dataset?
* Is this variable spatially correlated?
* How would you describe the temporal variability of `gsp` for the state `CALIFORNIA`, and is this variable temporally correlated?
* compute a spatial error model for the year 1986, where you regress 
public capital stock on gross state product, and discuss the results.